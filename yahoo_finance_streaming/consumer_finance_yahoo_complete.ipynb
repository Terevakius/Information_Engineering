{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6ddeec",
   "metadata": {},
   "source": [
    "Welcome to one of the collaborative Spark environments in ZHAW. You are not yet connected to Sparky by default. However, the necessary code template makes this a quick process. Keep in mind that you are sharing both the Jupyter environment and the Sparky cluster with others. Custom Python packages on the notebook/Spark driver side are installed with %pip install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e946a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ticker = 'NESN.SW'\n",
    "path = '/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36281c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PORT = 10089"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2407a1ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~ Sparky module loaded ~~~\n",
      "[slash] initialised for indirect scaling\n",
      "[slash] augmented context for app sparknotebook-..., cores limit 2\n",
      "[slash:proxy] logging failed to 127.0.0.1\n",
      "[slash] no ability to scale; proceeding at good luck\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/16 12:59:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[slash] app running with 2 cores\n",
      "Attached to Sparky cluster context from sparky-ext as sparknotebook-....\n",
      "Requested 2 cores; real number might be less.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sparky\n",
    "\n",
    "import pyspark\n",
    "import slash\n",
    "import pyspark.sql\n",
    "sc = sparky.connect(\"sparknotebook-...\", 2)\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "beea71cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 12:59:28 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import socket, time\n",
    "\n",
    "# Too high share price increase in stock\n",
    "#=======================================\n",
    "\n",
    "# This code seeks through the NYSE stock exchange data to see which\n",
    "# transactions are unusually priced, as compared to the overall previous price average.\n",
    "\n",
    "# Look at the CSV input file to understand how the data is structured\n",
    "\n",
    "# exchange company date price_open price_high price_low price_close stock_volume price_adj_close timestamp\n",
    "# NYSE\tASP\t2001-12-31\t12.55\t12.8\t12.42\t12.8\t11300\t6.91 2018-02-14 13:16:44.550444\n",
    "   \n",
    "  \n",
    "# 1. Read input stream from socket (by default, sockets contain raw strings, which we must then parse in a structured format)\n",
    "\n",
    "lines = spark.readStream.format(\"socket\")\\\n",
    "  .option(\"host\", \"localhost\")\\\n",
    "  .option(\"port\", PORT)\\\n",
    "  .load()\n",
    "\n",
    "# 2. Split the lines by comma (and assign each resulting column a proper name to reflect its contents, using the alias function)\n",
    "# Note that wherever needed, we must also cast the resulting column to its proper type (e.g. Float for prices, instead of the default String)\n",
    "structuredStream = lines.select(\\\n",
    "  split(lines.value, \",\")[0].cast('Float').alias(\"open\"),\\\n",
    "  split(lines.value, \",\")[1].cast('Float').alias(\"high\"),\\\n",
    "  split(lines.value, \",\")[2].cast('Float').alias(\"low\"),\\\n",
    "  split(lines.value, \",\")[3].cast('Float').alias(\"close\"),\\\n",
    "  split(lines.value, \",\")[4].cast('Float').alias(\"adj_close\"),\\\n",
    "  split(lines.value, \",\")[5].cast('Int').alias(\"volume\"),\\\n",
    "  split(lines.value, \",\")[6].cast('Timestamp').alias(\"id\"),\\\n",
    "  split(lines.value, \",\")[7].cast('String').alias(\"date\"),\\\n",
    "  split(lines.value, \",\")[8].cast('String').alias(\"time\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73421681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[open: float, high: float, low: float, close: float, adj_close: float, volume: int, id: timestamp, date: string, time: string]\n"
     ]
    }
   ],
   "source": [
    "print(structuredStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc70c4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.group.GroupedData object at 0x7fd1e2e9ae10>\n"
     ]
    }
   ],
   "source": [
    "# Add watermark to the structuredStream DataFrame\n",
    "structuredStreamWithWatermark = structuredStream.withWatermark(\"id\", \"5 minutes\")\n",
    "\n",
    "# Define the windowed stream\n",
    "windowedStream = structuredStreamWithWatermark.groupBy(window(\"id\", \"5 minutes\", \"5 minutes\"))\n",
    "\n",
    "print(windowedStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f180945f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[window: struct<start:timestamp,end:timestamp>, open: float, high: float, low: float, close: float]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max, first, last\n",
    "\n",
    "agregationsStream = windowedStream.agg(\n",
    "    first(\"adj_close\").alias(\"open\"),\n",
    "    max(\"high\").alias(\"high\"),\n",
    "    min(\"low\").alias(\"low\"),\n",
    "    last(\"adj_close\").alias(\"close\")\n",
    ")\n",
    "\n",
    "print(agregationsStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ac25c6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 12:59:30 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6fede734-20a4-4325-9348-be905d09fab9. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "23/06/16 12:59:30 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "# 'complete' outputMode for aggregations and 'append' outputMode to get complete records\n",
    "\n",
    "streamingETLQuery =\\\n",
    "agregationsStream \\\n",
    "  .writeStream \\\n",
    "  .format(\"memory\") \\\n",
    "  .queryName(\"aggDF\") \\\n",
    "  .outputMode(\"append\")\\\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59932b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[window: struct<start:timestamp,end:timestamp>, open: float, high: float, low: float, close: float]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc\n",
    "display(agregationsStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23482a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running SQL (initialization may take a while)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of rows in df is: 340\n",
      "\n",
      "**************************************\n",
      "\n",
      "Iteration no. 1\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/1_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 432\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of rows in df is: 0\n",
      "\n",
      "**************************************\n",
      "\n",
      "Iteration no. 2\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/2_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 433\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 3\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/3_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 434\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 4\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/4_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 435\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 5\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/5_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 436\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 6\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/6_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 437\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 7\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/7_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 438\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 8\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/8_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 439\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 13:33:30 WARN TransportChannelHandler: Exception in connection from /103.178.229.195:58856\n",
      "java.lang.IllegalArgumentException: Too large frame: 4849910940755296249\n",
      "\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 9\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/9_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 440\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 10\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/10_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 441\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 13:42:10 WARN TransportChannelHandler: Exception in connection from /103.178.229.175:47628\n",
      "java.lang.IllegalArgumentException: Too large frame: 4849910940755296249\n",
      "\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "23/06/16 13:43:14 WARN TransportChannelHandler: Exception in connection from /173.82.142.226:34182\n",
      "java.lang.IllegalArgumentException: Too large frame: 4849910940755296249\n",
      "\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 11\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/11_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 442\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 12\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/12_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 443\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 13\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/13_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current number of windows processed in stream: 444\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 14\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/16 14:01:14 WARN TransportChannelHandler: Exception in connection from /185.170.144.3:64951\n",
      "java.lang.IllegalArgumentException: Too large frame: 216172984696569848\n",
      "\tat org.sparkproject.guava.base.Preconditions.checkArgument(Preconditions.java:119)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.decodeNext(TransportFrameDecoder.java:148)\n",
      "\tat org.apache.spark.network.util.TransportFrameDecoder.channelRead(TransportFrameDecoder.java:98)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:444)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:412)\n",
      "\tat io.netty.channel.DefaultChannelPipeline$HeadContext.channelRead(DefaultChannelPipeline.java:1410)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:440)\n",
      "\tat io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:420)\n",
      "\tat io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:919)\n",
      "\tat io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:166)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:788)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:724)\n",
      "\tat io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:650)\n",
      "\tat io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:562)\n",
      "\tat io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:997)\n",
      "\tat io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)\n",
      "\tat io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1589)\n",
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/14_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 445\n",
      "\n",
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************\n",
      "\n",
      "Iteration no. 15\n",
      "\n",
      "Sample data from streaming dataframe:\n",
      "\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|window                                    |open  |high  |low   |close |id                 |close_larger_open|count_close_larger_open|count_open_larger_close|upper_shadow_length|lower_shadow_length|body_length|\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "|{2023-06-12 09:00:00, 2023-06-12 09:05:00}|107.84|107.86|107.52|107.84|2023-06-12 09:05:00|false            |0                      |1                      |0.020004272        |0.3199997          |0.0        |\n",
      "|{2023-06-12 09:05:00, 2023-06-12 09:10:00}|107.82|107.9 |107.82|107.88|2023-06-12 09:10:00|true             |1                      |0                      |0.020004272        |0.0                |0.05999756 |\n",
      "+------------------------------------------+------+------+------+------+-------------------+-----------------+-----------------------+-----------------------+-------------------+-------------------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/conversion.py:251: FutureWarning: Passing unit-less datetime64 dtype to .astype is deprecated and will raise in a future version. Pass 'datetime64[ns]' instead\n",
      "  series = series.astype(t, copy=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start writing file... \n",
      "\n",
      "/home/ubuntu/work/abd-fs23/fuxseb01/project/stock_data/NESN.SW/15_NESN.SW_window.csv\n",
      "File written to drive \n",
      "\n",
      "Current number of windows processed in stream: 446\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New # of rows in df is: 0\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.0:>              (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 10217:==========>(182 + 2) / 200][Stage 10220:>              (0 + 0) / 1]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m(df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m     21\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew # of rows in df is: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(df\u001b[38;5;241m.\u001b[39mcount() \u001b[38;5;241m-\u001b[39m old_count) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m   \u001b[38;5;28;01mwhile\u001b[39;00m(old_count \u001b[38;5;241m==\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;66;03m# don't do anything while there is no new data\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     26\u001b[0m   \u001b[38;5;66;03m# update the total count in order to be able to use the condition above in the next iteration\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py:1193\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1171\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1172\u001b[0m \n\u001b[1;32m   1173\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1193\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import sys\n",
    "import pandas\n",
    "\n",
    "from pyspark.sql.functions import desc, col, avg, when\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import sum as spark_sum, lag\n",
    "\n",
    "\n",
    "iter = 1\n",
    "old_count = 0\n",
    "\n",
    "#SQL statement required to get all the contents of the stream (use the name assigned earlier)\n",
    "df = spark.sql(\"select * from aggDF\")\n",
    "print(\"Running SQL (initialization may take a while)...\")\n",
    "\n",
    "while True:\n",
    "\n",
    "  # only start computing once some data has been collected (note that the dataframe is automatically updated by Spark)\n",
    "  if(df.count() != 0):\n",
    "    print('New # of rows in df is: ' + str(df.count() - old_count) + '\\n')\n",
    "    while(old_count == df.count()):\n",
    "      # don't do anything while there is no new data\n",
    "      continue\n",
    "      \n",
    "    # update the total count in order to be able to use the condition above in the next iteration\n",
    "    old_count = df.count()\n",
    "    \n",
    "    print(\"**************************************\")\n",
    "    print(\"\\nIteration no. \"+ str(iter) + \"\\n\")\n",
    "    print(\"Sample data from streaming dataframe:\\n\")\n",
    "    \n",
    "    # set the time to be the end of the window (just to have a visual indication of time)\n",
    "    df = df.withColumn(\"id\", df.window.end)\n",
    "       \n",
    "    # we sort by time first, because by default there is no ordering in the streaming dataframe and we want to only show the most recent results\n",
    "    # the following allows showing the most recent 5 windows (the False parameter instructs Spark not to truncate the output)\n",
    "    df = df.sort(desc(\"id\"))\n",
    "    \n",
    "    # Define the window specification for the moving average calculation\n",
    "    #ma_window_1 = Window.partitionBy(\"id\").orderBy(desc(\"id\")).rowsBetween(-4, 0)\n",
    "    # Calculate the moving average and add it as a new column\n",
    "    #df = df.withColumn(\"MA5\", avg(col(\"close\")).over(ma_window_1))\n",
    "    \n",
    "    # Define the window specification for the moving average calculation\n",
    "    #ma_window_2 = Window.partitionBy(\"id\").orderBy(desc(\"id\")).rowsBetween(-14, 0)\n",
    "    # Calculate the moving average and add it as a new column\n",
    "    #df = df.withColumn(\"MA15\", avg(col(\"close\")).over(ma_window_2))\n",
    "    \n",
    "    # Add a column indicating whether close is larger than open\n",
    "    df = df.withColumn(\"close_larger_open\", when(col(\"close\") > col(\"open\"), True).otherwise(False))\n",
    "    # Define the window specification for counting consecutive windows where close_larger_open is True\n",
    "    consecutive_window = Window.partitionBy(\"id\").orderBy(desc(\"id\")).rowsBetween(Window.unboundedPreceding, 0)\n",
    "    # Add a column counting consecutive windows where close_larger_open is True\n",
    "    df = df.withColumn(\"count_close_larger_open\", spark_sum(when(col(\"close_larger_open\"), 1).otherwise(0)).over(consecutive_window))\n",
    "    df = df.withColumn(\"count_open_larger_close\", spark_sum(when(col(\"close_larger_open\"), 0).otherwise(1)).over(consecutive_window))\n",
    "    \n",
    "    # Calculate the lower shadow length and add it as a new column\n",
    "    df = df.withColumn(\"upper_shadow_length\", when(col(\"close_larger_open\"), col(\"high\") - col(\"close\")).otherwise(col(\"high\") - col(\"open\")))\n",
    "    # Calculate the lower shadow length and add it as a new column\n",
    "    df = df.withColumn(\"lower_shadow_length\", when(col(\"close_larger_open\"), col(\"open\") - col(\"low\")).otherwise(col(\"close\") - col(\"low\")))\n",
    "\n",
    "    df = df.withColumn(\"body_length\", when(col(\"close_larger_open\"), col(\"close\") - col(\"open\")).otherwise(col(\"open\") - col(\"close\")))\n",
    "       \n",
    "    # Select the columns you want to save\n",
    "    # Convert the 'window' struct column to a string representation\n",
    "    #df = df.withColumn(\"window_str\", concat(\n",
    "    #    to_utc_timestamp(col(\"window.start\"), \"GMT\"),\n",
    "    #    to_utc_timestamp(col(\"window.end\"), \"GMT\")\n",
    "    #))\n",
    "\n",
    "    df.show(2, False)\n",
    "        \n",
    "    # Remove the original 'window' column if desired\n",
    "    #df = df.drop(\"window\")\n",
    "    \n",
    "    # Convert Spark DF to Pandas DF\n",
    "    pandas_df = df.toPandas()\n",
    "    # Extract the timestamp from the id column\n",
    "    pandas_df['time_stamp'] = pandas_df['id'].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "    # Get the most recent time_stamp value\n",
    "    time_stamp = pandas_df['time_stamp'].iloc[0]\n",
    "    \n",
    "    # Calculate a Moving Average\n",
    "    pandas_df['MA5'] = pandas_df['open'].rolling(window=5).mean()\n",
    "    pandas_df['MA15'] = pandas_df['open'].rolling(window=15).mean()\n",
    "    \n",
    "    # Reorder the columns in the Pandas DataFrame\n",
    "    pandas_df = pandas_df[['window', 'open', 'high', 'low', 'close', 'id', 'MA5', 'MA15', 'close_larger_open',\n",
    "                           'count_close_larger_open', 'count_open_larger_close', 'upper_shadow_length',\n",
    "                           'lower_shadow_length', 'body_length', 'time_stamp']]\n",
    "    \n",
    "    # Drop window for readability of csv file\n",
    "    pandas_df = pandas_df.drop('window', axis=1)\n",
    "\n",
    "    # Save the file - create the directory if it doesn't exist\n",
    "    stock_directory = os.path.join(path, ticker)\n",
    "    if not os.path.exists(stock_directory):\n",
    "        os.makedirs(stock_directory)\n",
    "\n",
    "    file_name = '/' + str(iter) + '_' + ticker + '_window.csv'\n",
    "    print('Start writing file... \\n')\n",
    "    print(stock_directory + file_name)\n",
    "    pandas_df.to_csv(stock_directory + file_name, index=False) \n",
    "    print('File written to drive \\n')\n",
    "\n",
    "    print(\"Current number of windows processed in stream: \"+ str(df.count()) + \"\\n\")\n",
    "    \n",
    "    iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c2eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
